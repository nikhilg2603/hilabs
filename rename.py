# -*- coding: utf-8 -*-
"""rename.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ob0LeIuxhlVeKsbyXDADQ3ySwXvhXI8e
"""

import pandas as pd
import numpy as np

care_df=pd.read_csv("./train/care.csv")

diagnosis_df=pd.read_csv("./train/diagnosis.csv")

patient_df=pd.read_csv("./train/patient.csv")

visit_df=pd.read_csv("./train/visit.csv")

combined_df = care_df.merge(diagnosis_df, on='patient_id', how='inner') \
                 .merge(patient_df, on='patient_id', how='inner') \
                 .merge(visit_df, on='patient_id', how='inner')

risk_df=pd.read_csv("./train/risk.csv")

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

# Drop NaN values from risk_score if any
data = risk_df['risk_score'].dropna()

# Compute mean and standard deviation
mu, std = data.mean(), data.std()

# Plot histogram
plt.figure(figsize=(8, 5))
plt.hist(data, bins=1000, density=True, alpha=0.6, color='skyblue', label='Risk Score Distribution')

# Plot normal curve
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 1000)
p = norm.pdf(x, mu, std)
plt.plot(x, p, 'r', linewidth=2, label='Normal Curve')

# Labels
plt.title('Normal Distribution Fit for Risk Score')
plt.xlabel('Risk Score')
plt.ylabel('Density')
plt.legend()
plt.show()

merged_df = pd.merge(diagnosis_df, risk_df, on='patient_id', how='inner')

# Step 1: Get patient IDs where hot_spotter_chronic_flag == 't'
chronic_patients = patient_df.loc[
    patient_df['hot_spotter_chronic_flag'] == 't',
    'patient_id'
]

# Step 2: Filter risk_df for those patient IDs
chronic_risk_scores = risk_df[risk_df['patient_id'].isin(chronic_patients)]

# Step 3 (optional): Display only relevant columns
chronic_risk_scores = chronic_risk_scores[['patient_id', 'risk_score']]

# Step 4: See result
print(chronic_risk_scores)

chronic_patients = patient_df.loc[
    patient_df['hot_spotter_readmission_flag'] == 't',
    'patient_id'
]

# Step 2: Filter risk_df for those patient IDs
chronic_risk_scores = risk_df[risk_df['patient_id'].isin(chronic_patients)]

# Step 3 (optional): Display only relevant columns
chronic_risk_scores = chronic_risk_scores[['patient_id', 'risk_score']]

# Step 4: See result
print(chronic_risk_scores)

import matplotlib.pyplot as plt

# Assuming you already have chronic_risk_scores from earlier
plt.figure(figsize=(8,5))
plt.hist(risk_df['risk_score'], bins=50, color='teal', alpha=0.7, edgecolor='black')

# Labels and title
plt.title('Distribution of Risk Scores for Chronic Patients')
plt.xlabel('Risk Score')
plt.ylabel('Number of Patients')
plt.grid(alpha=0.3)
plt.show()

from scipy import stats
z = np.abs(stats.zscore(risk_df['risk_score']))
risk_df[z > 3]

care_df['care_count_per_patient'] = care_df.groupby('patient_id')['care_id'].transform('count')

care_df_new=care_df.drop('care_id',axis=1)

care_df_new=care_df.drop(['last_care_dt','next_care_dt'],axis=1)

care_df_new=care_df_new.drop('care_id',axis=1)

# Filter only LAB_TEST rows
lab_test_df = care_df_new[care_df_new['msrmnt_type'] == 'LAB TEST']

# Get all unique subtypes
subtypes = lab_test_df['msrmnt_sub_type'].unique()

# Create a column for each LAB_TEST subtype (1 if that subtype exists for the patient, else 0)
for subtype in subtypes:
    care_df_new[f'LAB_TEST_{subtype}'] = care_df_new.apply(
        lambda x: x['msrmnt_value'] if (x['msrmnt_type'] == 'LAB TEST' and x['msrmnt_sub_type'] == subtype) else 0,
        axis=1
    )
care_df_new

# Separate LAB TEST rows
lab_test_rows = care_df_new[care_df_new['msrmnt_type'] == 'LAB TEST']

# Keep only non-lab rows for encoding
non_lab_rows = care_df_new[care_df_new['msrmnt_type'] != 'LAB TEST']

# Perform one-hot encoding on non-lab rows
encoded_df = pd.get_dummies(non_lab_rows, columns=['msrmnt_sub_type'], prefix='SUBTYPE')

# Combine back with LAB TEST rows
care_df_new = pd.concat([encoded_df, lab_test_rows], ignore_index=True)

care_df_new = care_df_new.drop(columns=['msrmnt_sub_type','msrmnt_type','msrmnt_value'])

cols = [
    'SUBTYPE_BREAST CANCER',
    'SUBTYPE_CHOLESTEROL',
    'SUBTYPE_COLORECTAL CANCER',
    'SUBTYPE_DIABETES',
    'SUBTYPE_HYPERTENSION'
]

care_df_new[cols] = care_df_new[cols].fillna(False)

diagnosis_df['diagnosis_count_per_patient'] = diagnosis_df.groupby('patient_id')['diagnosis_id'].transform('count')

diagnosis_df = pd.get_dummies(diagnosis_df, columns=['condition_name'], prefix='COND')

diagnosis_df_new=diagnosis_df.drop(['diagnosis_id','condition_type','condition_description','is_chronic'],axis=1)

visit_counts = (
    visit_df
    .groupby(['patient_id', 'visit_type'])
    .size()
    .unstack(fill_value=0)
    .add_prefix('VISIT_')
    .reset_index()
)

visit_counts

# 1️⃣ Convert to datetime
visit_df['visit_start_dt'] = pd.to_datetime(visit_df['visit_start_dt'])
visit_df['visit_end_dt'] = pd.to_datetime(visit_df['visit_end_dt'])

# 2️⃣ Calculate number of days per visit
visit_df['visit_days'] = (visit_df['visit_end_dt'] - visit_df['visit_start_dt']).dt.days + 1

# 3️⃣ Get number of visits per patient per type
visit_counts = (
    visit_df
    .groupby(['patient_id', 'visit_type'])
    .size()
    .unstack(fill_value=0)
    .add_prefix('VISIT_COUNT_')
)

# 4️⃣ Get total days per patient per type
visit_days = (
    visit_df
    .groupby(['patient_id', 'visit_type'])['visit_days']
    .sum()
    .unstack(fill_value=0)
    .add_prefix('VISIT_DAYS_')
)

# 5️⃣ Merge both together
visit_summary = (
    visit_counts
    .merge(visit_days, left_index=True, right_index=True)
    .reset_index()
)

patient_conditions = (
    diagnosis_df_new.groupby('patient_id')[['COND_CANCER', 'COND_DIABETES', 'COND_HYPERTENSION']]
    .max()
    .assign(total_conditions=lambda x: x.sum(axis=1))
    .reset_index()
)

diagnosis_df_new=patient_conditions.drop('total_conditions',axis=1)

diagnosis_df_new = diagnosis_df_new.astype(int)

subtype_cols = [
    'SUBTYPE_BREAST CANCER',
    'SUBTYPE_CHOLESTEROL',
    'SUBTYPE_COLORECTAL CANCER',
    'SUBTYPE_DIABETES',
    'SUBTYPE_HYPERTENSION'
]

care_visits_per_patient = (
    care_df_new
    .groupby('patient_id')[subtype_cols]
    .sum()
    .reset_index()
)

lab_test_cols = [
    'LAB_TEST_HbA1c',
    'LAB_TEST_SYSTOLIC BLOOD PRESSURE',
    'LAB_TEST_DIASTOLIC BLOOD PRESSURE'
]

lab_test_avg = (
    care_df_new
    .groupby('patient_id')[lab_test_cols]
    .mean()
    .reset_index()
)

merged_df_care = pd.merge(care_visits_per_patient, lab_test_avg, on='patient_id', how='outer')

# Define columns
lab_test_cols = [
    'LAB_TEST_HbA1c',
    'LAB_TEST_SYSTOLIC BLOOD PRESSURE',
    'LAB_TEST_DIASTOLIC BLOOD PRESSURE'
]

subtype_cols = [
    'SUBTYPE_BREAST CANCER',
    'SUBTYPE_CHOLESTEROL',
    'SUBTYPE_COLORECTAL CANCER',
    'SUBTYPE_DIABETES',
    'SUBTYPE_HYPERTENSION'
]

# Create aggregation dictionary
agg_dict = {col: 'mean' for col in lab_test_cols}  # average lab tests
agg_dict.update({col: 'sum' for col in subtype_cols})  # sum subtype visits
agg_dict.update({'care_gap_ind': 'first', 'care_count_per_patient': 'first'})  # other columns

# Aggregate per patient
patient_summary = care_df_new.groupby('patient_id').agg(agg_dict).reset_index()

care_df_new=patient_summary.drop(['care_count_per_patient'],axis=1)

care_df_new['care_gap_ind'] = care_df_new['care_gap_ind'].map({'t': 1, 'f': 0})

cols_to_drop = [col for col in patient_df.columns if 'hot_spotter' in col]
patient_df_new = patient_df.drop(columns=cols_to_drop)

from functools import reduce

# List of your 4 dataframes
dfs = [care_df_new, diagnosis_df_new, patient_df_new, visit_summary]  # replace with your actual dataframe names

# Merge all dataframes on 'patient_id' using outer join
merged_df = reduce(lambda left, right: pd.merge(left, right, on='patient_id', how='outer'), dfs)

# Fill missing values with 0
merged_df = merged_df.fillna(0)

# Merge risk_df with merged_df on patient_id
final_df = pd.merge(merged_df, risk_df[['patient_id', 'risk_score']], on='patient_id', how='left')

# Optional: fill missing risk_score with 0 if some patients don't have a risk value
final_df['risk_score'] = final_df['risk_score'].fillna(0)

# Commented out IPython magic to ensure Python compatibility.
# %pip install xgboost

import numpy as np
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import mean_squared_error

# Add a small constant to avoid log(0)
final_df['log_risk_score'] = np.log1p(final_df['risk_score'])

# Features
X = final_df.drop(columns=['patient_id', 'risk_score', 'log_risk_score'])
# Target (log-transformed)
y = final_df['log_risk_score']

# Bin log_risk_score into 5 categories for stratification
y_strata = pd.qcut(y, q=5, labels=False)

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Commented out IPython magic to ensure Python compatibility.
# %pip install lightgbm

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import mean_squared_error

# Features and target
X = final_df.drop(columns=['patient_id', 'risk_score', 'log_risk_score'])
y = final_df['log_risk_score']

# Bin target for stratified sampling
y_strata = pd.qcut(y, q=5, labels=False)

# Stratified K-Fold
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Define models to compare
models = {
    'LightGBM': lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=5, random_state=42),
    'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=7, random_state=42),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42),
    'Ridge': Ridge(alpha=1.0)
}

# Dictionary to store mean RMSE for each model
rmse_results = {}

for name, model in models.items():
    rmse_scores = []

    for train_idx, test_idx in kf.split(X, y_strata):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        if name == 'LightGBM':
            # LightGBM with early stopping
            model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)],
                eval_metric='rmse',
                callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]
            )
        else:
            model.fit(X_train, y_train)

        y_pred = model.predict(X_test)
        rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))

    rmse_results[name] = np.mean(rmse_scores)

print("Mean RMSE for each model:")
for k, v in rmse_results.items():
    print(f"{k}: {v:.4f}")
# Ensure datetime
care_df['last_care_dt'] = pd.to_datetime(care_df['last_care_dt'], errors='coerce')
care_df['next_care_dt'] = pd.to_datetime(care_df['next_care_dt'], errors='coerce')

care_df['care_gap_days'] = (care_df['next_care_dt'] - care_df['last_care_dt']).dt.days
care_df['has_next_care'] = care_df['next_care_dt'].notna().astype(int)

# Visits
visit_df['visit_start_dt'] = pd.to_datetime(visit_df['visit_start_dt'], errors='coerce')
visit_df['visit_end_dt'] = pd.to_datetime(visit_df['visit_end_dt'], errors='coerce')

visit_df = visit_df.sort_values(['patient_id', 'visit_start_dt'])

# Assign first visit per patient
first_visit = (
    visit_df.groupby('patient_id')['visit_start_dt']
    .min()
    .rename('first_visit_dt')
    .reset_index()
)

visit_df = visit_df.merge(first_visit, on='patient_id', how='left')

# Compute periods (ensure non-negative & ignore NaT)
visit_df['period_6m'] = (
    ((visit_df['visit_start_dt'] - visit_df['first_visit_dt']).dt.days // 182)
    .clip(lower=0)                     # prevent negative bins
)

visit_df['period_6m'] = visit_df['period_6m'].fillna(0).astype(int)

freq_df = (
    visit_df.groupby(['patient_id', 'period_6m'])
    .size()
    .reset_index(name='visit_count')
)

# Pivot so each period becomes a column
freq_pivot = freq_df.pivot(index='patient_id', columns='period_6m', values='visit_count').fillna(0)

# Rename columns to meaningful names
freq_pivot.columns = [f'visit_count_6m_{col}' for col in freq_pivot.columns]

# Reset index to make patient_id a column
freq_pivot = freq_pivot.reset_index()

visit_df['visit_start_dt'] = pd.to_datetime(visit_df['visit_start_dt'])

# Step 1: Get first visit per patient
first_visit = visit_df.groupby('patient_id')['visit_start_dt'].min().reset_index()
first_visit = first_visit.rename(columns={'visit_start_dt': 'first_visit_dt'})

# Step 2: Merge first visit back
visit_df = visit_df.merge(first_visit, on='patient_id', how='left')

# Step 3: Compute 6-month period per patient
visit_df['period_6m'] = ((visit_df['visit_start_dt'] - visit_df['first_visit_dt']).dt.days // 182).astype(int)

# Step 4: Count visits per patient per 6-month period
freq_df = (
    visit_df
    .groupby(['patient_id', 'period_6m'])
    .size()
    .reset_index(name='visit_count')
)

# Step 5: Pivot so each period becomes a column
freq_pivot = freq_df.pivot(index='patient_id', columns='period_6m', values='visit_count').fillna(0)
freq_pivot.columns = [f'visit_count_6m_{col}' for col in freq_pivot.columns]
freq_pivot = freq_pivot.reset_index()

# Step 6: Add total visit counts per patient
total_visits = visit_df.groupby('patient_id').size().reset_index(name='total_visit_count')

# Merge total visits with the pivoted dataframe
patient_visit_features = freq_pivot.merge(total_visits, on='patient_id', how='left')

# Result: one row per patient, visit counts per 6-month period + total visits
print(patient_visit_features.head())

import pandas as pd

# Ensure datetime
visit_df['visit_start_dt'] = pd.to_datetime(visit_df['visit_start_dt'])

# Sort visits per patient
visit_df = visit_df.sort_values(by=['patient_id', 'visit_start_dt'])

# Compute gap between consecutive visits per patient
visit_df['prev_visit'] = visit_df.groupby('patient_id')['visit_start_dt'].shift(1)
visit_df['gap_days'] = (visit_df['visit_start_dt'] - visit_df['prev_visit']).dt.days

# Calculate average gap per patient
avg_gap = visit_df.groupby('patient_id')['gap_days'].mean().reset_index()
avg_gap = avg_gap.rename(columns={'gap_days': 'avg_visit_gap_days'})

# Fill NaN gaps (first visit) with 0 or leave as NaN if you prefer
avg_gap['avg_visit_gap_days'] = avg_gap['avg_visit_gap_days'].fillna(0)

print(avg_gap)

# Merge patient-level visit counts with final_df
final_df = final_df.merge(patient_visit_features, on='patient_id', how='left')

# Merge average visit gap per patient
final_df = final_df.merge(avg_gap, on='patient_id', how='left')

# Optional: fill any remaining NaN values with 0
final_df.fillna(0, inplace=True)

# Check result
print(final_df.head())

final_df['LAB_TEST_HbA1c'] = final_df['LAB_TEST_HbA1c'].replace(0, 5.7)

final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'] = final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'].replace(0, 120)

final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'] = final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'].replace(5.7, 80)

import pandas as pd

# Ensure date columns are in datetime format
visit_df['visit_start_dt'] = pd.to_datetime(visit_df['visit_start_dt'])
visit_df['visit_end_dt'] = pd.to_datetime(visit_df['visit_end_dt'])
# visit_df['follow_up_dt'] = pd.to_datetime(visit_df['follow_up_dt'])

# Sort by patient and visit start date
visit_df = visit_df.sort_values(['patient_id', 'visit_start_dt'])

# Calculate day gaps between consecutive visits for each patient
visit_df['prev_visit_start'] = visit_df.groupby('patient_id')['visit_start_dt'].shift(1)
visit_df['day_gap'] = (visit_df['visit_start_dt'] - visit_df['prev_visit_start']).dt.days

# Calculate average day gap for each patient
avg_gap = visit_df.groupby('patient_id')['day_gap'].mean().reset_index()
avg_gap.rename(columns={'day_gap':'avg_day_gap'}, inplace=True)

print(avg_gap)

# Ensure date columns are datetime if needed (optional, if using visit gaps)
# final_df['visit_start_dt'] = pd.to_datetime(final_df['visit_start_dt'])
# final_df['visit_end_dt'] = pd.to_datetime(final_df['visit_end_dt'])

# 1. Total visits and total visit days
final_df['total_visits'] = final_df['VISIT_COUNT_ER'] + final_df['VISIT_COUNT_INPATIENT'] + final_df['VISIT_COUNT_URGENT CARE']
final_df['total_visit_days'] = final_df['VISIT_DAYS_ER'] + final_df['VISIT_DAYS_INPATIENT'] + final_df['VISIT_DAYS_URGENT CARE']

# 2. Average days per visit
final_df['avg_days_per_visit'] = final_df['total_visit_days'] / final_df['total_visits'].replace(0,1)

# 3. Ratios of visit types
final_df['ER_to_total_visits'] = final_df['VISIT_COUNT_ER'] / final_df['total_visits'].replace(0,1)
final_df['Inpatient_to_total_visits'] = final_df['VISIT_COUNT_INPATIENT'] / final_df['total_visits'].replace(0,1)

# 4. Average days per visit type
final_df['avg_days_ER'] = final_df['VISIT_DAYS_ER'] / final_df['VISIT_COUNT_ER'].replace(0,1)
final_df['avg_days_inpatient'] = final_df['VISIT_DAYS_INPATIENT'] / final_df['VISIT_COUNT_INPATIENT'].replace(0,1)
final_df['avg_days_urgent'] = final_df['VISIT_DAYS_URGENT CARE'] / final_df['VISIT_COUNT_URGENT CARE'].replace(0,1)

# 5. Blood pressure ratio
final_df['BP_ratio'] = final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'] / final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'].replace(0,1)

# 6. Number of comorbidities
final_df['num_conditions'] = final_df[['COND_CANCER','COND_DIABETES','COND_HYPERTENSION']].sum(axis=1)

# 7. Age group (bins)
final_df['age_group'] = pd.cut(final_df['age'], bins=[0,30,45,60,75,100], labels=[1,2,3,4,5])

# 8. Interaction: age × diabetes
final_df['age_times_diabetes'] = final_df['age'] * final_df['COND_DIABETES']

# 9. High utilizer flag
final_df['high_utilizer'] = (final_df['total_visits'] > final_df['total_visits'].median()).astype(int)

# 10. Risk tier (quartiles)
final_df['risk_tier'] = pd.qcut(final_df['risk_score'], 4, labels=False)

import numpy as np
import pandas as pd

# -------------------------
# 1. Average gap between visits per patient (if visit_start_dt available)
# -------------------------
if 'visit_start_dt' in final_df.columns:
    final_df['visit_start_dt'] = pd.to_datetime(final_df['visit_start_dt'])
    final_df = final_df.sort_values(['patient_id', 'visit_start_dt'])
    final_df['prev_visit'] = final_df.groupby('patient_id')['visit_start_dt'].shift(1)
    final_df['day_gap'] = (final_df['visit_start_dt'] - final_df['prev_visit']).dt.days
    avg_gap = final_df.groupby('patient_id')['day_gap'].mean().reset_index()
    avg_gap.rename(columns={'day_gap':'avg_day_gap'}, inplace=True)
    final_df = final_df.merge(avg_gap, on='patient_id', how='left')

# -------------------------
# 2. Age × number of conditions
# -------------------------
final_df['age_times_num_conditions'] = final_df['age'] * final_df['num_conditions']

# -------------------------
# 3. HbA1c × diabetes
# -------------------------
final_df['HbA1c_times_diabetes'] = final_df['LAB_TEST_HbA1c'] * final_df['COND_DIABETES']

# -------------------------
# 4. Systolic – diastolic BP difference
# -------------------------
final_df['BP_diff'] = final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'] - final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE']

# -------------------------
# 5. Binary lab flags
# -------------------------
final_df['high_HbA1c'] = (final_df['LAB_TEST_HbA1c'] > 5.7).astype(int)
final_df['high_BP'] = ((final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'] > 130) |
                       (final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'] > 80)).astype(int)

# -------------------------
# 6. Proportion of long visits (ER days > 1)
# -------------------------
final_df['long_ER_ratio'] = (final_df['VISIT_DAYS_ER'] > 1).astype(int) / final_df['total_visits'].replace(0,1)

# -------------------------
# 7. Weighted visit days
# -------------------------
final_df['weighted_visit_days'] = (final_df['VISIT_DAYS_ER']*1 +
                                   final_df['VISIT_DAYS_INPATIENT']*2 +
                                   final_df['VISIT_DAYS_URGENT CARE']*1.5)

# -------------------------
# 8. Visit type diversity
# -------------------------
final_df['visit_type_diversity'] = ((final_df['VISIT_COUNT_ER']>0).astype(int) +
                                   (final_df['VISIT_COUNT_INPATIENT']>0).astype(int) +
                                   (final_df['VISIT_COUNT_URGENT CARE']>0).astype(int))



# -------------------------
# 11. Care gap × number of conditions
# -------------------------
final_df['caregap_times_conditions'] = final_df['care_gap_ind'] * final_df['num_conditions']

# -------------------------
# 12. Lab bins (categorical)
# -------------------------
final_df['HbA1c_bin'] = pd.cut(final_df['LAB_TEST_HbA1c'], bins=[0,5.7,6.5,10], labels=[0,1,2])
final_df['BP_bin'] = pd.cut(final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'], bins=[0,120,140,200], labels=[0,1,2])

import numpy as np
import pandas as pd

# Assuming 'final_df' is your DataFrame

# -----------------------------------------------------------------
# 1. Warning: Check for Target Leakage
# -----------------------------------------------------------------
# If 'risk_tier' was created FROM 'risk_score' or 'log_risk_score',
# it MUST be removed before training.
# Uncomment the line below if this is the case.
#
# if 'risk_tier' in final_df.columns:
#     final_df = final_df.drop(columns=['risk_tier'])
#     print("Dropped 'risk_tier' to prevent target leakage.")


# -----------------------------------------------------------------
# 2. Comorbidity and Interaction Features
# -----------------------------------------------------------------
print("Creating Comorbidity and Interaction features...")

# Specific Comorbidities
final_df['diabetes_and_hypertension'] = final_df['COND_DIABETES'] * final_df['COND_HYPERTENSION']
final_df['diabetes_and_cancer'] = final_df['COND_DIABETES'] * final_df['COND_CANCER']
final_df['cancer_and_hypertension'] = final_df['COND_CANCER'] * final_df['COND_HYPERTENSION']
final_df['triple_threat'] = final_df['COND_DIABETES'] * final_df['COND_HYPERTENSION'] * final_df['COND_CANCER']

# Condition + Lab Value Interactions
final_df['hypertension_and_high_BP'] = final_df['COND_HYPERTENSION'] * final_df['high_BP']
final_df['hypertension_and_BP_diff'] = final_df['COND_HYPERTENSION'] * final_df['BP_diff']

# Care Gap Interactions
final_df['caregap_and_high_HbA1c'] = final_df['care_gap_ind'] * final_df['high_HbA1c']
final_df['caregap_and_high_utilizer'] = final_df['care_gap_ind'] * final_df['high_utilizer']
final_df['caregap_and_diabetes'] = final_df['care_gap_ind'] * final_df['COND_DIABETES']

# Age + Utilization Interaction
final_df['age_and_ER_visits'] = final_df['age'] * final_df['VISIT_COUNT_ER']


# -----------------------------------------------------------------
# 3. Clinical & Lab-Based Features
# -----------------------------------------------------------------
print("Creating Clinical & Lab-Based features...")

# Mean Arterial Pressure (MAP)
final_df['MAP'] = (final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'] + 2 * final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE']) / 3


# [Image of Mean Arterial Pressure calculation formula]


# BP Categories (Categorical Feature)
# Using np.select for efficient binning
bp_conditions = [
    (final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'] < 120) & (final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'] < 80),
    (final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'].between(120, 129)) & (final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'] < 80),
    (final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'].between(130, 139)) | (final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'].between(80, 89)),
    (final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'] >= 140) | (final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'] >= 90),
    (final_df['LAB_TEST_SYSTOLIC BLOOD PRESSURE'] > 180) | (final_df['LAB_TEST_DIASTOLIC BLOOD PRESSURE'] > 120)
]
bp_choices = [
    1, # Normal
    2, # Elevated
    3, # Stage 1 HTN
    4, # Stage 2 HTN
    5  # Hypertensive Crisis
]
# default=0 to catch any NaNs or edge cases
final_df['BP_category'] = np.select(bp_conditions, bp_choices, default=0)


# [Image of Blood Pressure Categories Chart]


# HbA1c Categories (Categorical Feature)
hba1c_conditions = [
    (final_df['LAB_TEST_HbA1c'] < 5.7),
    (final_df['LAB_TEST_HbA1c'].between(5.7, 6.4)),
    (final_df['LAB_TEST_HbA1c'].between(6.5, 8.0)),
    (final_df['LAB_TEST_HbA1c'] > 8.0)
]
hba1c_choices = [
    1, # Normal
    2, # Prediabetes
    3, # Controlled Diabetes
    4  # Uncontrolled Diabetes
]
# default=0 to catch any NaNs
final_df['HbA1c_category'] = np.select(hba1c_conditions, hba1c_choices, default=0)


# -----------------------------------------------------------------
# 4. Trend & Recency Features (from visit_count_6m_...)
# -----------------------------------------------------------------
print("Creating Trend & Recency features...")

# List of the 6-month visit columns in order (oldest to newest)
# Adjust this list if your column names or order are different
visit_cols = [
    'visit_count_6m_0', 'visit_count_6m_1', 'visit_count_6m_2',
    'visit_count_6m_3', 'visit_count_6m_4'
]

# Ensure these columns exist before proceeding
if all(col in final_df.columns for col in visit_cols):
    # Visit Trend (Slope)
    # We use np.polyfit to find the slope of the line of best fit
    periods = np.arange(len(visit_cols))

    # Define a function to apply to each row
    def get_slope(row):
        # We need at least 2 non-NaN points to fit a line
        if row.notna().sum() < 2:
            return np.nan
        # Fit a 1-degree polynomial (a line) and return the slope (the first coefficient)
        # We use .dropna() to handle any missing periods
        valid_indices = row.notna()
        return np.polyfit(periods[valid_indices], row[valid_indices], 1)[0]

    # Use .apply() along axis=1 (rows)
    final_df['visit_trend_slope'] = final_df[visit_cols].apply(get_slope, axis=1)

    # Recent vs. Past Utilization (assuming _3 and _4 are most recent)
    final_df['recent_visits'] = final_df['visit_count_6m_3'] + final_df['visit_count_6m_4']
    final_df['past_visits'] = final_df['visit_count_6m_0'] + final_df['visit_count_6m_1']
    final_df['recent_vs_past_diff'] = final_df['recent_visits'] - final_df['past_visits']

    # Visit Volatility (Standard Deviation of visits)
    final_df['visit_volatility'] = final_df[visit_cols].std(axis=1)

    # Zero-Visit Periods
    final_df['zero_visit_periods_count'] = (final_df[visit_cols] == 0).sum(axis=1)
else:
    print("Warning: Not all 'visit_count_6m_...' columns were found. Skipping Trend features.")


# -----------------------------------------------------------------
# 5. Advanced Utilization Ratios
# -----------------------------------------------------------------
print("Creating Advanced Utilization Ratios...")

# Add a small epsilon to denominators to prevent division by zero
epsilon = 1e-6

# High-Acuity Visit Ratio
final_df['high_acuity_visits'] = final_df['VISIT_COUNT_ER'] + final_df['VISIT_COUNT_INPATIENT']
final_df['high_acuity_ratio'] = final_df['high_acuity_visits'] / (final_df['total_visit_count'] + epsilon)

# High-Acuity Days Ratio
final_df['high_acuity_days'] = final_df['VISIT_DAYS_ER'] + final_df['VISIT_DAYS_INPATIENT']
final_df['high_acuity_days_ratio'] = final_df['high_acuity_days'] / (final_df['total_visit_days'] + epsilon)

# Urgent Care Ratio
final_df['urgent_care_ratio'] = final_df['VISIT_COUNT_URGENT CARE'] / (final_df['total_visit_count'] + epsilon)


print("\nFeature creation complete.")
print(f"New features added: {list(final_df.columns[-18:])}") # Show the last 18 new columns

from sklearn.feature_selection import mutual_info_regression
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np


final_df_cleaned = final_df.copy()
# --- Prepare Data ---
# 1. Define your features (X) and target (y)
features_to_exclude = ['patient_id', 'risk_score', 'log_risk_score']
valid_features_to_exclude = [col for col in features_to_exclude if col in final_df_cleaned.columns]
X = final_df_cleaned.drop(columns=valid_features_to_exclude)
y = final_df_cleaned['log_risk_score']

# 2. Identify which features are discrete (for the MI algorithm)
# We'll do this *before* encoding them all to numbers
discrete_features = []
for col in X.columns:
    # Treat object types and low-cardinality numbers as discrete
    if X[col].dtype == 'object' or pd.api.types.is_categorical_dtype(X[col]) or X[col].nunique() < 20:
        discrete_features.append(True)
    else:
        discrete_features.append(False)

print(f"Identified {sum(discrete_features)} discrete features.")

# 3. Robustly handle NaNs and Encode all columns to be numeric
for col in X.columns:
    if pd.api.types.is_numeric_dtype(X[col]) and not discrete_features[X.columns.get_loc(col)]:
        # A) For continuous numeric columns: Fill with median (more robust to outliers than 0)
        X[col] = X[col].fillna(X[col].median())
    else:
        # B) For discrete, categorical, or object columns:

        # If it's already a category dtype, add 'Missing' as a valid category
        if pd.api.types.is_categorical_dtype(X[col]):
            if 'Missing' not in X[col].cat.categories:
                X[col] = X[col].cat.add_categories(['Missing'])

        # Fill NaNs (for object or category) with the 'Missing' string
        X[col] = X[col].fillna('Missing')

        # Now, convert the entire column to category codes (numbers)
        X[col] = pd.Categorical(X[col]).codes

# At this point, all columns in X are numeric (int or float) and have no NaNs.

# --- Calculate MI ---
print("Calculating Mutual Information scores...")
mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=42)

# --- View Results ---
mi_series = pd.Series(mi_scores, index=X.columns)
mi_series = mi_series.sort_values(ascending=False)

print("\n--- Mutual Information Scores (Feature vs. Target) ---")
print(mi_series)

# Plot the top 30
plt.figure(figsize=(10, 8))
mi_series.head(30).plot(kind='barh')
plt.title('Top 30 Features by Mutual Information with log_risk_score')
plt.xlabel('Mutual Information Score')
plt.show()

# # List of all columns to remove
# features_to_remove = [
#     # 1. Target Leakage & Identifiers
#     'risk_score',
#     'risk_tier',

#     # 2. Redundant Aggregates
#     'total_visits',
#     'total_visit_count',
#     'total_visit_days',
#     'high_acuity_visits',
#     'high_acuity_days',

#     # 3. Redundant Time-Series
#     'visit_count_6m_0',
#     'visit_count_6m_1',
#     'visit_count_6m_2',
#     'visit_count_6m_3',
#     'visit_count_6m_4',
#     'recent_visits',
#     'past_visits',
#     'recent_vs_past_diff',

#     # 4. Redundant Binned/Categorical
#     'age_group',
#     'HbA1c_bin',
#     'high_HbA1c',
#     'BP_bin',
#     'high_BP',

#     # 5. Redundant Engineered Labs
#     'LAB_TEST_SYSTOLIC BLOOD PRESSURE',
#     'LAB_TEST_DIASTOLIC BLOOD PRESSURE',
#     'BP_ratio'
# ]

# # Create the cleaned DataFrame
# # errors='ignore' will prevent an error if a column was already removed
# final_df_cleaned = final_df.drop(columns=features_to_remove, errors='ignore')

# # This is your final list of features for training (X)
# # You still need to separate your target variable (y)
# X_features_list = [col for col in final_df_cleaned.columns if col not in ['log_risk_score']]

# print(f"Original number of columns: {len(final_df.columns)}")
# print(f"Number of columns removed: {len(features_to_remove)}")
# print(f"Remaining columns (features + target): {len(final_df_cleaned.columns)}")
# print("\nFinal list of features to use for X:")
# print(X_features_list)

# # Now you can proceed with your train-test split
# # X = final_df_cleaned[X_features_list]
# # y = final_df_cleaned['log_risk_score']

# Commented out IPython magic to ensure Python compatibility.
# %pip install catboost

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Select numeric features (excluding target)
X = final_df_cleaned.select_dtypes(include=['int64','float64']).drop(columns=['log_risk_score','patient_id', 'risk_tier','risk_score'])
y = final_df_cleaned['log_risk_score']

# Split into train/test (optional, for importance calculation)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# LightGBM dataset
lgb_train = lgb.Dataset(X_train, y_train)

# Train a simple LightGBM regressor
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'verbosity': -1,
    'boosting_type': 'gbdt',
    'seed': 42
}

model = lgb.train(params, lgb_train, num_boost_round=100)

# Get feature importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importance()
}).sort_values(by='importance', ascending=False)

# Select top k features
k = 30
top_features_lgb = feature_importance.head(k)['feature'].tolist()
print("Top", k, "features by LightGBM importance:", top_features_lgb)

# Optional: Visualize
import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.barh(feature_importance.head(k)['feature'], feature_importance.head(k)['importance'])
plt.xlabel('Feature Importance')
plt.title(f'Top {k} Features by LightGBM Importance')
plt.gca().invert_yaxis()
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Models
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
from sklearn.ensemble import RandomForestRegressor

# -------------------------
# 1. Prepare data
# -------------------------
X = final_df_cleaned[top_features_lgb]
y = final_df_cleaned['log_risk_score']  # target is log_risk_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -------------------------
# 2a. Define models
# -------------------------
models = {
    'LightGBM': lgb.LGBMRegressor(objective='regression', n_estimators=500, learning_rate=0.05, random_state=42),
    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', n_estimators=500, learning_rate=0.05, random_state=42),
    'CatBoost': CatBoostRegressor(iterations=500, learning_rate=0.05, verbose=0, random_state=42),
    'RandomForest': RandomForestRegressor(n_estimators=500, random_state=42)
}

# -------------------------
# 3. Train, predict and evaluate on original scale
# -------------------------
results = []

for name, model in models.items():
    # Train
    model.fit(X_train, y_train)

    # Predict on log scale
    y_pred_log = model.predict(X_test)

    # Convert predictions back to original risk score scale
    y_pred_orig = np.exp(y_pred_log)
    y_test_orig = np.exp(y_test)

    # Evaluate
    rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))
    r2 = r2_score(y_test_orig, y_pred_orig)

    results.append({'Model': name, 'RMSE': rmse, 'R2': r2})

    print(f"{name} -> Original Scale RMSE: {rmse:.4f}, R2: {r2:.4f}")

# -------------------------
# 4. Compare all models
# -------------------------
results_df = pd.DataFrame(results)
results_df = results_df.sort_values(by='RMSE')
print("\nModel Comparison on Original Risk Score Scale:")
print(results_df)

# --- Feature correlation matrix (numeric features only; excludes target) ---

import numpy as np
import pandas as pd

TARGET_COL = "risk_score"      # change if your target name differs
CORR_METHOD = "pearson"        # or "spearman"

# Copy and prep
df = final_df_cleaned.copy()

# Exclude target
X = df.drop(columns=[c for c in df.columns if c == TARGET_COL], errors="ignore")

# Treat booleans as ints and keep numeric columns
for c in X.columns:
    if X[c].dtype == bool:
        X[c] = X[c].astype(int)
work_num = X.select_dtypes(include=[np.number]).copy()

# Correlation matrix
corr = work_num.corr(method=CORR_METHOD)

# (Optional) upper triangle mask to avoid duplicates when listing pairs
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))

# Preview
corr.head()  # or: display(corr)

# --- List highly correlated pairs above a threshold ---

HIGH_CORR_THRESH = 0.8  # adjust as needed

# If 'upper' not defined from the previous block, compute it:
if "upper" not in globals():
    upper = work_num.corr(method=CORR_METHOD).where(
        np.triu(np.ones(work_num.shape[1]), k=1).astype(bool)
    )

high_pairs = (
    upper.stack()
         .rename("corr")
         .rename_axis(["feat1", "feat2"])
         .reset_index()
)
high_pairs["abs_corr"] = high_pairs["corr"].abs()
high_pairs = high_pairs.query("abs_corr >= @HIGH_CORR_THRESH") \
                       .sort_values("abs_corr", ascending=False) \
                       .reset_index(drop=True)

# Preview top pairs
high_pairs.head(30)  # or: display(high_pairs)

# --- Drop a redundant feature from each high-corr pair
#     Keep the more predictive wrt target; else fewer NaNs; else higher variance ---

# y for predictiveness comparison; if missing, set y=None
y = df[TARGET_COL] if TARGET_COL in df.columns else None

def _choose_drop_feature(feat1, feat2, Xnum, yvec):
    s1, s2 = Xnum[feat1], Xnum[feat2]

    # (a) target predictiveness (absolute Pearson corr with target)
    if yvec is not None:
        yt = pd.to_numeric(yvec, errors="coerce")
        c1 = abs(pd.concat([s1, yt], axis=1).corr(method="pearson").iloc[0, 1])
        c2 = abs(pd.concat([s2, yt], axis=1).corr(method="pearson").iloc[0, 1])
        c1 = -1 if pd.isna(c1) else c1
        c2 = -1 if pd.isna(c2) else c2
        if c1 != c2:
            return feat2 if c1 > c2 else feat1  # drop the less predictive

    # (b) fewer NaNs
    n1, n2 = s1.isna().sum(), s2.isna().sum()
    if n1 != n2:
        return feat1 if n1 > n2 else feat2  # drop the one with MORE NaNs

    # (c) higher variance (keep higher variance; drop lower)
    v1, v2 = s1.var(skipna=True), s2.var(skipna=True)
    if not pd.isna(v1) and not pd.isna(v2) and v1 != v2:
        return feat2 if v1 > v2 else feat1

    # (d) tie-breaker: drop lexicographically later
    return max(feat1, feat2)

to_drop = []
seen_pairs = set()

for _, row in high_pairs.iterrows():
    f1, f2 = row["feat1"], row["feat2"]
    if f1 in to_drop or f2 in to_drop:
        continue
    key = tuple(sorted((f1, f2)))
    if key in seen_pairs:
        continue
    seen_pairs.add(key)
    drop_feat = _choose_drop_feature(f1, f2, work_num, y)
    to_drop.append(drop_feat)

# De-correlated feature set
X_decorrelated = work_num.drop(columns=to_drop, errors="ignore").copy()

# If you need target back:
final_for_model = (
    pd.concat([X_decorrelated, y], axis=1)
    if y is not None else X_decorrelated
)

# Quick summary
print("Dropped features due to high inter-feature correlation:", len(to_drop))
print(sorted(to_drop)[:50])
print("Original numeric features:", work_num.shape[1])
print("After dropping:", X_decorrelated.shape[1])

X_decorrelated.columns

X_decorrelated=X_decorrelated.drop(columns='log_risk_score')

from sklearn.impute import SimpleImputer
X = X_decorrelated.copy()
y = final_df_cleaned.loc[X.index, 'log_risk_score'].copy()

print(f"X shape: {X.shape} | y length: {len(y)}")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)

# Handle NaNs for sklearn RF (and to keep things consistent across models)
imp = SimpleImputer(strategy="median")
X_train_i = imp.fit_transform(X_train)
X_test_i  = imp.transform(X_test)

# -------------------------
# 2) Define models
# -------------------------
models = {
    # 'LightGBM': lgb.LGBMRegressor(objective='regression', n_estimators=500, learning_rate=0.05, random_state=42),
    # 'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', n_estimators=500, learning_rate=0.05,
    #                             max_depth=6, subsample=0.9, colsample_bytree=0.9, random_state=42, n_jobs=-1),
    'CatBoost': CatBoostRegressor(iterations=500, learning_rate=0.05, loss_function='RMSE', verbose=0, random_state=42),
    # 'RandomForest': RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)
}

y_pred_log = model.predict(X_test_i)
y_pred = np.exp(y_pred_log)  # convert back to original scale

risk_output = pd.DataFrame({
    'patient_id': df['patient_id'],
    'predicted_risk_score': y_pred
})

risk_output.to_csv('risk.csv', index=False)
print(f"Saved risk predictions to {'risk.csv'}")



# -------------------------
# 3) Train, predict, evaluate on ORIGINAL scale
# -------------------------
results = []

for name, model in models.items():
    # Train on imputed matrices (CatBoost/LightGBM/XGB handle NaNs, but we keep it uniform)
    model.fit(X_train_i, y_train)

    # Predict on log scale
    y_pred_log = model.predict(X_test_i)

    # Convert back to original risk score scale
    # If you used log1p for target, replace exp with expm1 for both y_pred and y_test.
    y_pred_orig = np.exp(y_pred_log)
    y_test_orig = np.exp(y_test)

    rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))
    r2 = r2_score(y_test_orig, y_pred_orig)

    results.append({'Model': name, 'RMSE': rmse, 'R2': r2})
    print(f"{name} -> Original Scale RMSE: {rmse:.4f}, R2: {r2:.4f}")

# -------------------------
# 4) Compare all models
# -------------------------
results_df = pd.DataFrame(results).sort_values(by='RMSE', ascending=True).reset_index(drop=True)
print("\nModel Comparison on Original Risk Score Scale:")
print(results_df.to_string(index=False))